from WindPy import w
import pandas_datareader.data as web
import pandas as pd
import os
import numpy as np
import yfinance as yf
import warnings

pd.options.mode.chained_assignment = None
warnings.filterwarnings('ignore', category=FutureWarning)


# ===================== é…ç½® =====================
NEW_DATE="2026-2-20"
CONFIG = {
    "end_date": "2026-02-05",
    "long_path": r"/Users/zjy/python/ETF/ETFè·Ÿè¸ªæŒ‡æ•°é‡ä»·æ•°æ®-éæ—¥åº¦æ›´æ–°/",
    "short_path": r"/Users/zjy/python/ETF/ETFè·Ÿè¸ªæŒ‡æ•°é‡ä»·æ•°æ®-æ—¥åº¦æ›´æ–°/",
    "wind_params": {
        "Days": "Trading",        # ä»…æŠ“å–äº¤æ˜“æ—¥æ•°æ®
        "Fill": "Blank",          # ç©ºå€¼ï¼ˆä»£ç å±‚ä¸é¢å¤–å¤„ç†ï¼‰
        "Order": "D",             # æ•°æ®æŒ‰æ—¥æœŸé™åºè¿”å›
        "Period": "D",            # æ—¥é¢‘æ•°æ®
        "TradingCalendar": "SSE", # ä»¥ä¸Šäº¤æ‰€äº¤æ˜“æ—¥å†ä¸ºå‡†
        "Currency": "Original",   # åŸå§‹è´§å¸å•ä½
        "PriceAdj": "U"           # ä»·æ ¼ä¸å¤æƒCalendar": "SSE", "Currency": "Original", "PriceAdj": "U"
    },
    "symbols": {
    "980092.CNI": "2013-01-02",    # è‡ªç”±ç°é‡‘æµæŒ‡æ•°
    "h30269.CSI": "2018-05-11",    # çº¢åˆ©ä½æ³¢æŒ‡æ•°
    "000905.SH": "2005-01-04",     # ä¸­è¯500æŒ‡æ•°
    "932000.CSI": "2014-01-02",    # ä¸­è¯2000æŒ‡æ•°
    "000688.SH": "2020-01-02",     # ç§‘åˆ›50æŒ‡æ•°
    "000300.SH": "2005-01-04",     # æ²ªæ·±300æŒ‡æ•°
    "HSHYLV.HI": "2017-05-22",     # æ’ç”Ÿæ¸¯è‚¡é€šé«˜è‚¡æ¯ä½æ³¢åŠ¨æŒ‡æ•°
    "HSBIO.HI": "2019-12-16",      # æ’ç”Ÿç”Ÿç‰©ç§‘æŠ€æŒ‡æ•°
    "HSTECH.HI": "2020-07-27",     # æ’ç”Ÿç§‘æŠ€æŒ‡æ•°
    "930709.CSI": "2020-02-24",    # é¦™æ¸¯è¯åˆ¸æŒ‡æ•°
    # "DJI.GI": "1997-11-07",        # é“ç¼æ–¯å·¥ä¸šå¹³å‡æŒ‡æ•°
    "000201.CZC": "2010-03-02",    # æ˜“ç››èƒ½åŒ–AæŒ‡æ•°
    "NH0700.NHF": "2018-06-29",    # å—åæœ‰è‰²é‡‘å±æŒ‡æ•°
    "NH0015.NHF": "2004-06-01",    # å—åè±†ç²•æŒ‡æ•°
    "AU.SHF": "2008-01-09",        # SHFEé»„é‡‘æœŸè´§
    "511380.SH": "2020-04-07",     # å¯è½¬å€ºETFï¼ˆåšæ—¶ï¼‰
    "511020.SH": "2019-02-22",     # å›½å€ºETFï¼ˆ5è‡³10å¹´ï¼‰
    "511260.SH": "2017-08-24",     # åå¹´å›½å€ºETF
    "511220.SH": "2014-12-16",     # åŸæŠ•å€ºETFï¼ˆæµ·å¯Œé€šï¼‰
    "159972.SZ": "2019-11-08",     # 5å¹´åœ°å€ºETF
    "159816.SZ": "2020-09-04"      # 0-4å¹´åœ°å€ºETF
    }
}

EXTERNAL_SYMBOLS = {
    "^DJI": "2006-01-04",      # é“ç¼æ–¯å·¥ä¸šæŒ‡æ•°
    "^SPX": "2006-01-04",      # S&P 500 æŒ‡æ•°
    # "^NKX": "2006-01-04",      # æ—¥ç»225æŒ‡æ•°
    # "1321.JP": "2007-07-30",   # æ—¥ç»225 ETF (é‡æ‘)
    "^DAX": "2006-01-04",      # å¾·å›½DAXæŒ‡æ•°
    "EXS1.DE": "2007-01-04"    # å¾·å›½DAX ETF (iShares)
}

YAHOO_SYMBOLS = {
    "^N225": "2006-01-04",      # æ—¥ç»225æŒ‡æ•°ï¼ˆç°è´§ï¼‰
}


# ===================== å·¥å…·å‡½æ•° =====================
def _wind_opts(params: dict) -> str:
    """ç”ŸæˆWind APIå‚æ•°å­—ç¬¦ä¸²"""
    return ";".join(f"{k}={v}" for k, v in params.items())

def _save_df(df: pd.DataFrame, symbol: str, path: str):
    """ä¿å­˜DataFrameä¸ºCSVï¼ˆè‡ªåŠ¨åˆ›å»ºç›®å½•ï¼‰"""
    os.makedirs(path, exist_ok=True)  # â† ç›´æ¥å†…è”
    fp = os.path.join(path, f"{symbol}.csv")
    df.fillna("").to_csv(fp, index=False, encoding="utf-8-sig")
    print(f"âœ… {symbol}: å·²ä¿å­˜è‡³ {fp}")

def _read_latest_date(symbol: str, path: str) -> str:
    """ä»å·²æœ‰CSVè¯»å–windæœ€æ–°æ—¥æœŸï¼ˆç”¨äºå¢é‡èµ·ç‚¹ï¼‰"""
    fp = os.path.join(path, f"{symbol}.csv")
    if not os.path.exists(fp):
        return CONFIG["symbols"][symbol]
    try:
        date_str = pd.read_csv(fp, usecols=["date"], nrows=1).iloc[0]["date"]
        return pd.to_datetime(date_str).strftime("%Y-%m-%d")
    except Exception:
        return CONFIG["symbols"][symbol]
    
def _read_external_latest_date(symbol: str, path: str) -> str:
    """ä»å·²æœ‰CSVè¯»å–æµ·å¤–æ ‡çš„æœ€æ–°æ—¥æœŸï¼ˆç”¨äºå¢é‡èµ·ç‚¹ï¼‰"""
    fp = os.path.join(path, f"{symbol}.csv")
    if not os.path.exists(fp):
        return EXTERNAL_SYMBOLS[symbol]
    try:
        date_str = pd.read_csv(fp, usecols=["date"], nrows=1).iloc[0]["date"]
        return pd.to_datetime(date_str).strftime("%Y-%m-%d")
    except Exception:
        return EXTERNAL_SYMBOLS[symbol]
    

def _align_to_target_dates(raw_df: pd.DataFrame, symbol: str, target_dates: pd.Series, start_date: str) -> pd.DataFrame:
    """
    å°† raw_df æŒ‰ç…§ target_datesï¼ˆ000300.SH çš„ date åˆ—ï¼‰å¯¹é½
    - åªä¿ç•™ >= start_date çš„æ—¥æœŸ
    - ç¼ºå¤±æ—¥æœŸç”¨ç©ºå€¼å¡«å……ï¼ˆä¿æŒç»“æ„ï¼‰
    """

    # ç¡®ä¿ raw_df çš„ date æ˜¯å­—ç¬¦ä¸²ï¼ˆä¸ target ä¸€è‡´ï¼‰
    raw_df = raw_df.copy()
    raw_df["date"] = pd.to_datetime(raw_df["date"])
    target_dates_dt = pd.to_datetime(target_dates)

    # ç­›é€‰ target ä¸­ >= start_date çš„æ—¥æœŸ
    aligned_dates = target_dates_dt[target_dates_dt >= pd.to_datetime(start_date)].copy()

    # æ„å»ºå®Œæ•´æ—¥æœŸæ¡†æ¶
    aligned_df = pd.DataFrame({"date": aligned_dates})
    
    # å·¦è¿æ¥åŸå§‹æ•°æ®ï¼ˆä¿ç•™æ‰€æœ‰ç›®æ ‡æ—¥æœŸï¼Œç¼ºå¤±å¡«ç©ºï¼‰
    aligned_df = aligned_df.merge(raw_df, on="date", how="left")

    # è½¬å›å­—ç¬¦ä¸²æ ¼å¼
    aligned_df["date"] = aligned_df["date"].dt.strftime("%Y-%m-%d")
    
    return aligned_df

def _reprocess_nulls_for_aligned(df: pd.DataFrame, end: str) -> pd.DataFrame:
    """
    å¯¹å·²å¯¹é½çš„ DataFrame é‡æ–°æ‰§è¡Œç©ºå€¼å¤„ç†ï¼š
    - åˆ é™¤ end å½“å¤©çš„ç©ºå€¼è¡Œ
    - å¯¹ end ä¹‹å‰çš„ç©ºå€¼ç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥å¡«å……ï¼ˆæ³¨æ„ï¼šdf æ˜¯é™åºï¼ï¼‰
    """
    df = df.copy()
    df["date"] = pd.to_datetime(df["date"])
    end_date = pd.to_datetime(end).date()

    price_cols = ["open", "close", "high", "low"]
    vol_col = "volume"
    all_cols = price_cols + [vol_col]

    # æ ‡è®°ç©ºå€¼è¡Œ
    mask_null = df[all_cols].isnull().any(axis=1)


    # åˆ é™¤ end å½“å¤©çš„ç©ºå€¼è¡Œ
    is_end_day = df["date"].dt.date == end_date
    to_delete = mask_null & is_end_day
    if to_delete.any():
        deleted_dates = df[to_delete]["date"].dt.strftime("%Y-%m-%d").unique()
        df = df[~to_delete].copy()
        for d in deleted_dates:
            print(f"âš ï¸ å¯¹é½ååˆ é™¤: {d} å«ç©ºå€¼ï¼ˆend_dateï¼‰")

    if df.empty:
        return df

    # æ³¨æ„ï¼šdf æ˜¯é™åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰ï¼Œæ‰€ä»¥è¦ç”¨ bfillï¼ˆç”¨ä¸‹ä¸€è¡Œå¡«å……å½“å‰ç©ºå€¼ = ç”¨æ›´æ—©çš„äº¤æ˜“æ—¥å¡«å……ï¼‰
    was_null = df[all_cols].isnull().any(axis=1)
    df[all_cols] = df[all_cols].fillna(method='bfill')
    now_valid = ~df[all_cols].isnull().any(axis=1)
    filled_mask = was_null & now_valid
    if filled_mask.any():
        filled_dates = df[filled_mask]["date"].dt.strftime("%Y-%m-%d").unique()
        # for d in filled_dates:
        #     print(f"âš ï¸ å¯¹é½åå¡«å……: {d} ç©ºå€¼å·²ç”¨å‰ä¸€æ—¥æ•°æ®è¡¥å……")

    # æ ¼å¼æ ‡å‡†åŒ–ï¼ˆç¡®ä¿å­—ç¬¦ä¸²æ ¼å¼ï¼‰
    for col in price_cols:
        df[col] = df[col].astype(float).apply(lambda x: f"{x:.4f}")
    df["volume"] = (
        df["volume"]
        .astype(float)
        .round()
        .fillna(0)
        .astype(int)
        .astype(str)
    )
    df["date"] = df["date"].dt.strftime("%Y-%m-%d")
    return df

# ===================== æ•°æ®æŠ“å–ä¸æ¸…æ´— =====================
def _fetch_clean(symbol: str, start: str, end: str, params: dict) -> pd.DataFrame | None:
    """ä»Windè·å–å¹¶æ¸…æ´—é‡ä»·æ•°æ®ï¼ˆopen/close/high/low/volumeï¼‰"""
    raw = w.wsd(symbol, "open,close,high,low,volume", start, end, _wind_opts(params))
    if raw.ErrorCode != 0 or not raw.Data:
        print(f"âš ï¸ {symbol}: æ— æœ‰æ•ˆæ•°æ®")
        return None

    df = pd.DataFrame(raw.Data, index=["open","close","high","low","volume"], columns=raw.Times).T
    df.reset_index(inplace=True)
    df.columns = ["date", "open", "close", "high", "low", "volume"]
    df["date"] = pd.to_datetime(df["date"])
    df.replace([np.inf, -np.inf], np.nan, inplace=True)


# æ ‡è®°å«ç©ºå€¼çš„è¡Œ
    price_cols = ["open", "close", "high", "low"]
    vol_col = "volume"
    # æ ‡è®°å«ç©ºå€¼çš„è¡Œ
    mask_null = df[price_cols + [vol_col]].isnull().any(axis=1)
    if mask_null.any():
        end_date = pd.to_datetime(end).date()
        is_end_day = df["date"].dt.date == end_date

        # åˆ é™¤ end å½“å¤©çš„ç©ºå€¼è¡Œ
        to_delete = mask_null & is_end_day
        if to_delete.any():
            deleted_dates = df[to_delete]["date"].dt.strftime("%Y-%m-%d").unique()
            df = df[~to_delete].copy()
            for d in deleted_dates:
                print(f"âš ï¸ {symbol}: {d} å«ç©ºå€¼ï¼Œå·²åˆ é™¤æ­¤è¡Œ")

        # å…³é”®ï¼šWind æ˜¯é™åº â†’ ç”¨ bfill å®ç°â€œç”¨å‰ä¸€ä¸ªäº¤æ˜“æ—¥ï¼ˆæ—¶é—´ä¸Šæ›´æ—©ï¼‰å¡«å……â€
        if not df.empty:
            was_null = df[price_cols + [vol_col]].isnull().any(axis=1)
            # åœ¨é™åºæ•°æ®ä¸­ï¼Œbfill = ç”¨ä¸‹ä¸€è¡Œï¼ˆæ—¶é—´æ›´æ—©ï¼‰å¡«å……å½“å‰ç©ºå€¼
            df[price_cols + [vol_col]] = df[price_cols + [vol_col]].fillna(method='bfill')
            
            # æ£€æŸ¥å“ªäº›ç©ºå€¼è¢«æˆåŠŸå¡«å……
            now_valid = ~df[price_cols + [vol_col]].isnull().any(axis=1)
            filled_mask = was_null & now_valid
            if filled_mask.any():
                filled_dates = df[filled_mask]["date"].dt.strftime("%Y-%m-%d").unique()
                for d in filled_dates:
                    print(f"âš ï¸ {symbol}: {d} å«ç©ºå€¼ï¼Œå·²è¡¥å……ä¸ºå‰ä¸€æ—¥æ•°æ®")

    if df.empty:
        return None

    # æ ¼å¼æ ‡å‡†åŒ–ï¼šä»·æ ¼4ä½å°æ•°ï¼Œæˆäº¤é‡æ•´æ•°
    for col in ["open","close","high","low"]:
        df[col] = df[col].astype(float).apply(lambda x: f"{x:.4f}")

    # ç„¶åå†æ‰§è¡Œè½¬æ¢
    df["volume"] = df["volume"].astype(float).round().astype(int).astype(str)
    df["date"] = df["date"].dt.strftime("%Y-%m-%d")

    print(f"âœ… {symbol}: æŠ“å– {len(df)} æ¡æœ‰æ•ˆæ•°æ®")
    return df

def _fetch_clean_stooq(name: str, symbol: str, start: str, end: str) -> pd.DataFrame | None:
    """
    ä» Stooq è·å–å¹¶æ¸…æ´—æµ·å¤–æŒ‡æ•°/ETF æ•°æ®ï¼Œä»…åšåŸºç¡€æ¸…æ´—ï¼Œä¿ç•™ NaNï¼Œä¸è½¬å­—ç¬¦ä¸²ã€‚
    æ‰€æœ‰æ ¼å¼åŒ–å’Œç©ºå€¼å¤„ç†ç”± _reprocess_nulls_for_aligned ç»Ÿä¸€æ‰§è¡Œã€‚
    """
    try:
        df = web.DataReader(symbol, 'stooq', start, end)
    except Exception as e:
        print(f"âš ï¸ {name} ({symbol}): Stooq ä¸‹è½½å¤±è´¥ - {e}")
        return None

    if df.empty:
        print(f"âš ï¸ {name} ({symbol}): æ— æ•°æ®è¿”å›")
        return None

    df.columns = [col.lower() for col in df.columns]
    required_cols = ["open", "close", "high", "low", "volume"]
    if not all(col in df.columns for col in required_cols):
        print(f"âš ï¸ {name} ({symbol}): ç¼ºå°‘å¿…è¦åˆ—")
        return None

    df = df.reset_index()
    df.rename(columns={df.columns[0]: "date"}, inplace=True)
    df.columns = [col.lower() for col in df.columns]
    df = df[["date", "open", "close", "high", "low", "volume"]].copy()
    df["date"] = pd.to_datetime(df["date"])
    df.replace([np.inf, -np.inf], np.nan, inplace=True)

    # === å…³é”®ï¼šåªè½¬æ•°å€¼ç±»å‹ï¼Œä¿ç•™ NaNï¼Œä¸è½¬å­—ç¬¦ä¸²ï¼===
    for col in ["open", "close", "high", "low", "volume"]:
        df[col] = pd.to_numeric(df[col], errors='coerce')  # éæ•°å­— â†’ NaN

    # æŒ‰æ—¥æœŸé™åºï¼ˆä¸ Wind ä¸€è‡´ï¼‰
    df.sort_values("date", ascending=False, inplace=True)

    if df.empty:
        return None

    print(f"âœ… {name} ({symbol}): æŠ“å– {len(df)} æ¡åŸå§‹æ•°æ®")
    return df  # æ³¨æ„ï¼šæ‰€æœ‰ä»·æ ¼/volume ä»æ˜¯ floatï¼Œå« NaN

def _fetch_clean_yahoo(name: str, symbol: str, start: str, end_date: str) -> pd.DataFrame | None:
    """
    ä» Yahoo Finance è·å–å¹¶æ¸…æ´—æµ·å¤–æŒ‡æ•°/ETF æ•°æ®ã€‚
    æ³¨æ„ï¼šend_date æ˜¯åŒ…å«çš„ï¼ˆå³ä¼šè¿”å› end_date å½“å¤©çš„æ•°æ®ï¼‰
    """
    # === å…³é”®ä¿®æ”¹ï¼šæŠŠ end_date åŠ  1 å¤©ï¼Œå› ä¸º yfinance çš„ end æ˜¯ä¸åŒ…å«çš„ ===
    end_for_yahoo = (pd.to_datetime(end_date) + pd.Timedelta(days=1)).strftime("%Y-%m-%d")
    
    try:
        df = yf.download(
            symbol,
            start=start,
            end=end_for_yahoo,   # â† ä½¿ç”¨åŠ äº†1å¤©çš„æ—¥æœŸ
            threads=False,
            progress=False,
            auto_adjust=False,
            keepna=True
        )
    except Exception as e:
        print(f"âš ï¸ {name} ({symbol}): Yahoo ä¸‹è½½å¤±è´¥ - {e}")
        return None

    if df.empty:
        print(f"âš ï¸ {name} ({symbol}): æ— æ•°æ®è¿”å›")
        return None

    # å¤„ç† MultiIndex åˆ—ï¼ˆå¦‚ ('Close', '^N225')ï¼‰
    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.get_level_values(0)

    df.columns = [col.lower() for col in df.columns]

    required_cols = ["open", "close", "high", "low", "volume"]
    if not all(col in df.columns for col in required_cols):
        print(f"âš ï¸ {name} ({symbol}): ç¼ºå°‘å¿…è¦åˆ—ï¼Œå®é™…åˆ—: {list(df.columns)}")
        return None

    df = df.reset_index()
    df.rename(columns={df.columns[0]: "date"}, inplace=True)
    df = df[["date", "open", "close", "high", "low", "volume"]].copy()
    df["date"] = pd.to_datetime(df["date"])
    df.replace([np.inf, -np.inf], np.nan, inplace=True)

    for col in ["open", "close", "high", "low", "volume"]:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # è½¬ä¸ºé™åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
    df.sort_values("date", ascending=False, inplace=True)

    if df.empty:
        return None

    print(f"âœ… {name} ({symbol}): æŠ“å– {len(df)} æ¡åŸå§‹æ•°æ®")
    return df

# ===================== ä¸»æµç¨‹ =====================
def _fetch_all(symbols: dict[str, str], end: str, params: dict) -> dict[str, pd.DataFrame]:
    """æ‰¹é‡æŠ“å–å¤šä¸ªæ ‡çš„"""
    return {
        sym: df for sym, df in (
            (sym, _fetch_clean(sym, start, end, params))
            for sym, start in symbols.items()
        ) if df is not None
    }

def generate_long_data():
    """ç”Ÿæˆå…¨é‡é•¿å‘¨æœŸæ•°æ®ï¼ˆä¿å­˜åˆ° long_pathï¼‰"""
    w.start()
    if not w.isconnected():
        raise RuntimeError("âŒ Wind æœªè¿æ¥")
    data = _fetch_all(CONFIG["symbols"], CONFIG["end_date"], CONFIG["wind_params"])
    for sym, df in data.items():
        _save_df(df, sym, CONFIG["long_path"])
    w.stop()

def update_short_data(new_end: str = NEW_DATE):
    """å¢é‡æ›´æ–°ï¼šåŸºäº long_path æœ€æ–°æ—¥æœŸæŠ“æ–°æ•°æ®ï¼Œæ‹¼æ¥åå­˜å…¥ short_path"""
    w.start()
    if not w.isconnected():
        raise RuntimeError("âŒ Wind æœªè¿æ¥")

    # ç¡®å®šæ¯ä¸ªæ ‡çš„çš„å¢é‡èµ·å§‹æ—¥
    starts = {sym: _read_latest_date(sym, CONFIG["long_path"]) for sym in CONFIG["symbols"]}
    new_data = _fetch_all(starts, new_end, CONFIG["wind_params"])

    for sym, short_df in new_data.items():
        long_fp = os.path.join(CONFIG["long_path"], f"{sym}.csv")
        if not os.path.exists(long_fp):
            continue
        long_df = pd.read_csv(long_fp, encoding="utf-8-sig")
        # æ‹¼æ¥ + å»é‡ + é™åº
        combined = pd.concat([long_df, short_df], ignore_index=True)
        combined.drop_duplicates("date", keep="first", inplace=True)
        combined["date"] = pd.to_datetime(combined["date"])
        combined.sort_values("date", ascending=False, inplace=True)
        combined["date"] = combined["date"].dt.strftime("%Y-%m-%d")
        _save_df(combined, sym, CONFIG["short_path"])

    w.stop()

def generate_external_long_data(end_date: str = CONFIG["end_date"]):
    """ç”Ÿæˆæµ·å¤–æŒ‡æ•°å…¨é‡æ•°æ®ï¼ˆä¿å­˜åˆ° long_pathï¼‰"""
    print("\nğŸŒ å¼€å§‹ä¸‹è½½ Stooq å…¨é‡æµ·å¤–æŒ‡æ•°æ•°æ®...\n")
    results = {}
    for symbol, start in EXTERNAL_SYMBOLS.items():
        name = symbol  # å¯é€‰ï¼šåŠ ä¸ªæ˜ å°„è¡¨ç¾åŒ–æ—¥å¿—
        df = _fetch_clean_stooq(name, symbol, start, end_date)
        if df is not None:
            results[symbol] = df

    for symbol, df in results.items():
        _save_df(df, symbol, CONFIG["long_path"])

def update_external_short_data(new_end: str = NEW_DATE):
    """å¢é‡æ›´æ–°æµ·å¤–æŒ‡æ•°æ•°æ®ï¼ˆåŸºäº long_path æœ€æ–°æ—¥æœŸï¼Œå­˜å…¥ short_pathï¼‰"""
    print("\nğŸ”„ å¢é‡æ›´æ–° Stooq æµ·å¤–æŒ‡æ•°æ•°æ®...\n")
    
    # === æ–°å¢ï¼šè¯»å– 000300.SH ä½œä¸ºæ—¥æœŸåŸºå‡† ===
    target_fp = os.path.join(CONFIG["short_path"], "000300.SH.csv")
    if not os.path.exists(target_fp):
        raise FileNotFoundError(f"âŒ åŸºå‡†æ–‡ä»¶ä¸å­˜åœ¨: {target_fp}ï¼Œè¯·å…ˆç”Ÿæˆæ²ªæ·±300æ—¥åº¦æ•°æ®")
    target_df = pd.read_csv(target_fp, usecols=["date"], encoding="utf-8-sig")
    target_dates = target_df["date"]

    starts = {sym: _read_external_latest_date(sym, CONFIG["long_path"]) for sym in EXTERNAL_SYMBOLS}
    new_data = {}
    for symbol, start in starts.items():
        name = symbol
        df = _fetch_clean_stooq(name, symbol, start, new_end)
        if df is not None:
            new_data[symbol] = df

    for symbol, short_df in new_data.items():
        long_fp = os.path.join(CONFIG["long_path"], f"{symbol}.csv")
        if not os.path.exists(long_fp):
            continue
        long_df = pd.read_csv(long_fp, encoding="utf-8-sig")
        
        # åˆå¹¶æ–°æ—§æ•°æ®
        combined = pd.concat([long_df, short_df], ignore_index=True)
        combined.drop_duplicates("date", keep="first", inplace=True)
        combined["date"] = pd.to_datetime(combined["date"])
        combined.sort_values("date", ascending=False, inplace=True)
        combined["date"] = combined["date"].dt.strftime("%Y-%m-%d")

        # === æ–°å¢ï¼šæ­¥éª¤1 - æŒ‰ 000300.SH æ—¥æœŸå¯¹é½ ===
        start_date = EXTERNAL_SYMBOLS[symbol]
        aligned_df = _align_to_target_dates(combined, symbol, target_dates, start_date)

        # === æ–°å¢ï¼šæ­¥éª¤2 - é‡æ–°æ‰§è¡Œç©ºå€¼å¤„ç†ï¼ˆé’ˆå¯¹å¯¹é½åçš„æ•°æ®ï¼‰===
        aligned_df = _reprocess_nulls_for_aligned(aligned_df, new_end)

        # === ä¿å­˜ ===
        _save_df(aligned_df, symbol, CONFIG["short_path"])

def generate_yahoo_long_data(end_date: str = CONFIG["end_date"]):
    """ç”Ÿæˆ Yahoo æµ·å¤–æŒ‡æ•°å…¨é‡æ•°æ®ï¼ˆä¿å­˜åˆ° long_pathï¼‰"""
    print("\nğŸŒ å¼€å§‹ä¸‹è½½ Yahoo å…¨é‡æµ·å¤–æŒ‡æ•°æ•°æ®...\n")
    
    results = {}
    for symbol, start in YAHOO_SYMBOLS.items():
        name = symbol
        df = _fetch_clean_yahoo(name, symbol, start, end_date)
        if df is not None:
            results[symbol] = df

    for symbol, df in results.items():
        _save_df(df, symbol, CONFIG["long_path"])


# ===================== Yahoo å¢é‡æ›´æ–° =====================
def update_yahoo_short_data(new_end: str = NEW_DATE):
    """å¢é‡æ›´æ–° Yahoo æµ·å¤–æŒ‡æ•°æ•°æ®ï¼ˆåŸºäº long_path æœ€æ–°æ—¥æœŸï¼Œå­˜å…¥ short_pathï¼‰"""
    print("\nğŸ”„ å¢é‡æ›´æ–° Yahoo æµ·å¤–æŒ‡æ•°æ•°æ®...\n")
    
    # === è¯»å–åŸºå‡†æ—¥æœŸï¼ˆ000300.SHï¼‰===
    target_fp = os.path.join(CONFIG["short_path"], "000300.SH.csv")
    if not os.path.exists(target_fp):
        raise FileNotFoundError(f"âŒ åŸºå‡†æ–‡ä»¶ä¸å­˜åœ¨: {target_fp}ï¼Œè¯·å…ˆç”Ÿæˆæ²ªæ·±300æ—¥åº¦æ•°æ®")
    target_df = pd.read_csv(target_fp, usecols=["date"], encoding="utf-8-sig")
    target_dates = target_df["date"]


    # è¯»å–æ¯ä¸ªæ ‡çš„çš„å¢é‡èµ·ç‚¹
    starts = {}
    for sym in YAHOO_SYMBOLS:
        fp = os.path.join(CONFIG["long_path"], f"{sym}.csv")
        if os.path.exists(fp):
            try:
                date_str = pd.read_csv(fp, usecols=["date"], nrows=1).iloc[0]["date"]
                starts[sym] = pd.to_datetime(date_str).strftime("%Y-%m-%d")
            except Exception:
                starts[sym] = YAHOO_SYMBOLS[sym]
        else:
            starts[sym] = YAHOO_SYMBOLS[sym]

    # ä¸‹è½½å¢é‡æ•°æ®
    new_data = {}
    for symbol, start in starts.items():
        name = symbol
        df = _fetch_clean_yahoo(name, symbol, start, new_end)
        if df is not None:
            new_data[symbol] = df

    # åˆå¹¶ã€å¯¹é½ã€å¤„ç†ç©ºå€¼ã€ä¿å­˜
    for symbol, short_df in new_data.items():
        long_fp = os.path.join(CONFIG["long_path"], f"{symbol}.csv")
        if not os.path.exists(long_fp):
            continue
        long_df = pd.read_csv(long_fp, encoding="utf-8-sig")
        
        # åˆå¹¶å»é‡
        combined = pd.concat([long_df, short_df], ignore_index=True)
        combined.drop_duplicates("date", keep="first", inplace=True)
        combined["date"] = pd.to_datetime(combined["date"])
        combined.sort_values("date", ascending=False, inplace=True)
        combined["date"] = combined["date"].dt.strftime("%Y-%m-%d")

        # å¯¹é½åˆ° 000300.SH äº¤æ˜“æ—¥å†
        start_date = YAHOO_SYMBOLS[symbol]
        aligned_df = _align_to_target_dates(combined, symbol, target_dates, start_date)

        # ç»Ÿä¸€ç©ºå€¼å¤„ç†ï¼ˆå¡«å…… + åˆ é™¤ end_day ç©ºå€¼ï¼‰
        aligned_df = _reprocess_nulls_for_aligned(aligned_df, new_end)

        # ä¿å­˜
        _save_df(aligned_df, symbol, CONFIG["short_path"])

# ===================== æ‰§è¡Œå…¥å£ =====================
if __name__ == "__main__":
    # Wind æ•°æ®
    # generate_long_data()
    update_short_data()

    # Stooq æµ·å¤–æ•°æ®
    # generate_external_long_data(end_date=CONFIG["end_date"])
    # update_external_short_data(new_end=NEW_DATE)

    # generate_yahoo_long_data(end_date=CONFIG["end_date"])
    # update_yahoo_short_data(new_end=NEW_DATE)

    